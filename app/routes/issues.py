import asyncio
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from app.schemas import ChatRequest
from app.storage import gemini_client, sf_client, groq_client, get_youtube_videos, save_chat_to_db

router = APIRouter(prefix="/chat", tags=["AI Agents"])

@router.post("/cofounder")
async def cofounder_chat(request: ChatRequest):
    async def generate():
        full_response = ""  # AI á€•á€¼á€±á€¬á€á€™á€»á€¾ á€…á€¯á€‘á€¬á€¸á€–á€­á€¯á€·
        winner_model = "Unknown"  
        
        asyncio.create_task(
            asyncio.to_thread(save_chat_to_db, request.user_id, "user", request.message, "cofounder")
        )
        
        CoFounder_system_prompt = (
            "You are an expert strategic co-founder. "
            "If the user says 'hi', 'hello', or greets you without a specific idea, "
            "respond warmly and ask them what startup or project they are thinking about. "
            "If the user provides a specific idea or goal, provide a professional, "
            "step-by-step roadmap to help them launch it. Keep your tone encouraging and professional."
        )
        
        # ğŸš€ áá‹ Video á€›á€¾á€¬á€á€²á€· logic á€€á€­á€¯ á€…á€…á€ºá€‘á€¯á€á€ºá€™á€šá€º
        # User message á€€ á€¡á€”á€Šá€ºá€¸á€†á€¯á€¶á€¸ á€…á€€á€¬á€¸á€œá€¯á€¶á€¸ áƒ á€œá€¯á€¶á€¸á€‘á€€á€º á€•á€­á€¯á€™á€¾ (á€¥á€•á€™á€¬ "build laundry app") á€—á€®á€’á€®á€šá€­á€¯ á€›á€¾á€¬á€™á€šá€º
        is_greeting = any(word in request.message.lower() for word in ["hi", "hello", "hey"])
        words_count = len(request.message.split())
        
        video_task = None
        # Greeting á€™á€Ÿá€¯á€á€ºá€˜á€² á€…á€€á€¬á€¸á€œá€¯á€¶á€¸ áƒ á€œá€¯á€¶á€¸á€‘á€€á€ºá€•á€­á€¯á€™á€¾ YouTube á€€á€­á€¯ á€á€±á€«á€ºá€™á€šá€º
        if not is_greeting and words_count > 2:
            video_task = asyncio.create_task(
                asyncio.to_thread(get_youtube_videos, f"{request.message} business roadmap 2025")
            )
        
        # ğŸï¸ Race Logic: Gemini á€”á€²á€· Groq á€€á€­á€¯ á€•á€¼á€­á€¯á€„á€ºá€á€­á€¯á€„á€ºá€¸á€™á€šá€º
        async def call_gemini():
            try:
     
                response = await gemini_client.aio.models.generate_content(
                    model='gemini-3-flash-preview',
                    contents=request.message,
                    config={'system_instruction': CoFounder_system_prompt}
                )
                return (response.text, "Gemini")
        
            except Exception as e:
                print(f"Gemini Error: {e}")
                return (None, "Gemini")

        async def call_groq():
            try:
                # Groq (Llama 3.3) á€€ á€¡á€›á€™á€ºá€¸á€™á€¼á€”á€ºá€œá€­á€¯á€· Backup á€¡á€–á€¼á€…á€º á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸á€•á€«
                response = await groq_client.chat.completions.create(
                    model="llama-3.3-70b-versatile",
                    messages=[
                        {"role": "system", "content": CoFounder_system_prompt},
                        {"role": "user", "content": request.message}
                    ]
                )
                return (response.choices[0].message.content, "Groq")
            except Exception as e:
                print(f"Groq Error: {e}")
                return (None, "Groq")

        # 2. Task á€™á€»á€¬á€¸á€€á€­á€¯ á€á€•á€¼á€­á€¯á€„á€ºá€”á€€á€º á€…á€á€„á€ºá€á€¼á€„á€ºá€¸ (Do NOT await here)
        t1 = asyncio.create_task(call_gemini())
        t2 = asyncio.create_task(call_groq())
        pending = {t1, t2}
        
        winner_result = None
        
        # 3. Race Loop: á€¡á€–á€¼á€±á€›á€á€²á€·á€¡á€‘á€­ (á€á€­á€¯á€·) Task á€€á€¯á€”á€ºá€á€²á€·á€¡á€‘á€­ á€…á€±á€¬á€„á€·á€ºá€™á€šá€º
        while pending:
            # á€•á€‘á€™á€†á€¯á€¶á€¸ á€•á€¼á€®á€¸á€á€²á€·á€€á€±á€¬á€„á€ºá€€á€­á€¯ á€šá€°á€™á€šá€º
            done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
            
            for task in done:
                result, model_name = task.result()  # Tuple á€€á€­á€¯ unpack á€œá€¯á€•á€ºá€™á€šá€º
                if result: # á€¡á€–á€¼á€±á€™á€¾á€”á€ºá€€á€”á€ºá€…á€½á€¬ á€›á€á€²á€·á€›á€„á€º
                    winner_result = result
                    winner_model = model_name   # á€˜á€šá€ºá€á€°á€”á€­á€¯á€„á€ºá€á€½á€¬á€¸á€œá€² á€™á€¾á€á€ºá€‘á€¬á€¸á€™á€šá€º
                    break
            
            if winner_result:
                break # á€¡á€–á€¼á€±á€›á€•á€¼á€®á€†á€­á€¯á€›á€„á€º á€€á€»á€”á€ºá€á€¬á€€á€­á€¯ á€†á€€á€ºá€™á€…á€±á€¬á€„á€·á€ºá€á€±á€¬á€·á€˜á€°á€¸

        # á€€á€»á€”á€ºá€”á€±á€á€²á€· Task á€á€½á€±á€€á€­á€¯ á€–á€»á€€á€ºá€á€­á€™á€ºá€¸á€™á€šá€º (Resource á€™á€–á€¼á€¯á€”á€ºá€¸á€¡á€±á€¬á€„á€º)
        for p in pending: p.cancel()

        # Developer á€¡á€á€½á€€á€º Terminal á€™á€¾á€¬ á€•á€¼á€•á€±á€¸á€á€¼á€„á€ºá€¸
        print(f"ğŸš€ Race Winner: {winner_model}")
            
        # 4. Result á€•á€¼á€”á€ºá€•á€­á€¯á€·á€á€¼á€„á€ºá€¸
        if winner_result:
            full_response = winner_result
            yield winner_result
        else:
            fallback_msg = "âš ï¸ Both AI services are currently busy. Please try again."
            full_response = fallback_msg
            yield fallback_msg
        
        # 5. Video Logic Checking
        if video_task:
            try:
                videos = await video_task 
                # AI á€›á€²á€· á€¡á€–á€¼á€±á€‘á€²á€™á€¾á€¬ keyword á€á€½á€±á€•á€«á€™á€¾ Video á€•á€¼á€™á€šá€º
                has_roadmap = any(x in full_response.lower() for x in ["roadmap", "step 1", "strategy", "launch"])
                
                if videos and has_roadmap:
                    video_text = "\n\n### ğŸ“º Recommended Tutorials for your Roadmap:\n"
                    for v in videos:
                        video_text += f"- [{v['title']}]({v['link']})\n"
                    
                    full_response += video_text
                    yield video_text
            except Exception as e:
                print(f"Video Error: {e}")

        # 6. Save to DB
        asyncio.create_task(
            asyncio.to_thread(save_chat_to_db, request.user_id, "assistant", full_response, f"Co-founder({winner_model})")
        )

    return StreamingResponse(generate(), media_type="text/plain")

# --- Mentor (Async Streaming) ---
@router.post("/mentor")
async def mentor_chat(request: ChatRequest):
    async def generate():
        full_response = ""
        
        mentor_system_prompt = (
            "You are a wise and supportive Mentor. "
            "Rule 1: If the user just says 'hi', 'hello' or greets you, respond warmly, "
            "introduce yourself briefly as their mentor, and ask what's on their mind or what they want to learn. "
            "Rule 2: Do not give a long roadmap or advice unless they ask a specific question or share a goal. "
            "Rule 3: Use an encouraging, professional, yet friendly tone."
        )
        
        # Save User Message
        asyncio.create_task(
            asyncio.to_thread(save_chat_to_db, request.user_id, "user", request.message, "mentor")
        )

        try:
            # await á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€¼á€®á€¸ non-blocking á€á€±á€«á€ºá€šá€°á€™á€šá€º
            response = await sf_client.chat.completions.create(
                model="deepseek-ai/DeepSeek-V3",
                messages=[
                    {"role": "system", "content": mentor_system_prompt},
                    {"role": "user", "content": request.message}
                ],
                stream=True
            )
            
            # async for á€”á€²á€· á€á€…á€ºá€œá€¯á€¶á€¸á€á€»á€„á€ºá€¸á€…á€® stream á€œá€¯á€•á€ºá€•á€¼á€®á€¸ UI á€˜á€€á€ºá€€á€­á€¯ á€•á€­á€¯á€·á€™á€šá€º
            async for chunk in response:
                # DeepSeek client structure á€•á€±á€«á€ºá€™á€°á€á€Šá€ºá€•á€¼á€®á€¸ choices[0].delta.content á€€á€­á€¯ á€šá€°á€•á€«á€á€šá€º
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content
                    
            # Save AI Response
            # 2. AI á€–á€¼á€±á€á€¬ á€•á€¼á€®á€¸á€á€½á€¬á€¸á€™á€¾ Database á€™á€¾á€¬ á€á€­á€™á€ºá€¸á€™á€šá€º (Background Task)
            asyncio.create_task(
                asyncio.to_thread(
                    save_chat_to_db, 
                    request.user_id, 
                    "assistant", 
                    full_response, 
                    "mentor (DeepSeek-V3)"
                )
            )
            
        except Exception as e:
            print(f"Mentor Agent Error: {e}")
            yield f"Mentor Error: {str(e)}"

    return StreamingResponse(generate(), media_type="text/plain")

# --- Support (Async JSON Response) ---
@router.post("/support")
async def support_chat(request: ChatRequest):
    
    support_system_prompt = (
        "You are a helpful and professional Customer Support Assistant. "
        "Your goal is to provide clear, concise, and accurate information. "
        "If the user greets you, reply with a warm welcome and ask how you can assist them today. "
        "If they report an issue, be empathetic, acknowledge the problem, and offer a direct solution or next steps. "
        "Keep your responses short and professional."
    )
    
    # Save User Message    
    asyncio.create_task(
        asyncio.to_thread(save_chat_to_db, request.user_id, "user", request.message, "support")
    )
    
    try:
        # await á€á€¯á€¶á€¸á€œá€­á€¯á€€á€ºá€á€²á€·á€¡á€á€½á€€á€º á€’á€® API á€€ á€¡á€–á€¼á€±á€™á€•á€±á€¸á€á€„á€ºá€™á€¾á€¬ á€á€á€¼á€¬á€¸ request á€á€½á€±á€€á€­á€¯ á€œá€€á€ºá€á€¶á€”á€­á€¯á€„á€ºá€á€½á€¬á€¸á€•á€«á€•á€¼á€®
        response = await groq_client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": support_system_prompt},
                {"role": "user", "content": request.message}
            ]
        )
        
        reply = response.choices[0].message.content
        
        # 2. AI Response á€€á€­á€¯ Background á€™á€¾á€¬ á€á€­á€™á€ºá€¸á€™á€šá€º
        asyncio.create_task(
            asyncio.to_thread(
                save_chat_to_db, 
                request.user_id, 
                "assistant", 
                reply, 
                "support (Llama-3.1-8B)"
            )
        )
        
        return {"reply": reply}
    except Exception as e:
        print(f"Support Agent Error: {e}")
        return {"error": str(e)}